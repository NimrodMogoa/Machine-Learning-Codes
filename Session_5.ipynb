{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Session 5\n",
        "\n",
        "Natural Language Processing (NLP)\n",
        "\n",
        "Files used:\n",
        "* AutoAndElectronics.zip\n",
        "* FarmAds.csv\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Michael de la Maza\n",
        "\n",
        "AI/ML\n",
        "\n",
        "Hult International business School\n",
        "\n",
        "Adapted from \"Data Mining for Business Analytics\" by Shmueli"
      ],
      "metadata": {
        "id": "4csI0bk9Dp3T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dVmQXmlWDf59"
      },
      "outputs": [],
      "source": [
        "!pip install dmba"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from zipfile import ZipFile\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "soQpctYdHInv"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "BS_l8zitLtlc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tokenization, Stemming, Stop Words"
      ],
      "metadata": {
        "id": "MHP45dWuHPRB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Prints word frequencies in a document\n",
        "def printTermDocumentMatrix(vectorizer, matrix):\n",
        "\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "    count_array = matrix.toarray()\n",
        "    df = pd.DataFrame(data=count_array,columns = feature_names)\n",
        "\n",
        "    print(df)"
      ],
      "metadata": {
        "id": "AmLhjg2BJmjU"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: create count vectors\n",
        "\n",
        "text = ['the cat and the dog are playing',\n",
        "        'the cat is playing',\n",
        "        'hi! :)',\n",
        "        'i like ham & Eggs',\n",
        "        'the chicken plays with the eggs?']\n",
        "\n",
        "# Extract tokens from text\n",
        "# A token is lower and upper case plus !,:,)\n",
        "count_vect = CountVectorizer(token_pattern='[a-zA-Z!:)]+')\n",
        "counts = count_vect.fit_transform(text)\n",
        "\n",
        "printTermDocumentMatrix(count_vect, counts)\n",
        "\n",
        "# Notice that 'eggs?' does not appear\n"
      ],
      "metadata": {
        "id": "c0GakYZYHVk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5 minute exercise\n",
        "#Try with different sentences. Do you get what you expect?\n",
        "\n"
      ],
      "metadata": {
        "id": "nIPGr0DOLAZi"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5 minute exercise\n",
        "# Change CountVectorizer to include &. What does this do? Can you give an example?"
      ],
      "metadata": {
        "id": "2LtR2NraAh0A"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# filter out stop words\n",
        "count_vect = CountVectorizer(stop_words='english') # built in list of stop words\n",
        "counts = count_vect.fit_transform(text)\n",
        "\n",
        "printTermDocumentMatrix(count_vect, counts)\n",
        "\n",
        "feature_names = count_vect.get_feature_names_out()\n",
        "for i in range(counts.shape[0]): # will see a nicer way to do this later\n",
        "    print(\" \".join([feature_names[j] for j in counts[i].indices]))\n"
      ],
      "metadata": {
        "id": "KyQClFwwHM3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stem words\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "\n",
        "print('eggs => ', ps.stem('eggs'))\n",
        "print('running => ', ps.stem('running'))\n",
        "\n",
        "for i in range(counts.shape[0]):\n",
        "    words = [feature_names[j] for j in counts[i].indices]\n",
        "    stemmed_words = [ps.stem(word) for word in words]\n",
        "    stemmed_sentence = ' '.join(stemmed_words)\n",
        "    print(stemmed_sentence)"
      ],
      "metadata": {
        "id": "RmI5CbZPwZBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5 minute exercise\n",
        "# Use ps.stem to stem words of your choosing. Are any of the results unexpected?\n",
        "# arguing\n",
        "# arrangement & arrange\n",
        "# people"
      ],
      "metadata": {
        "id": "9hUOyQdmyfjN"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Case study: Internet discussion posts\n",
        "\n",
        "Classify posts as related to autos or electronics"
      ],
      "metadata": {
        "id": "mpmxfLyWy9ra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import dmba\n",
        "\n",
        "corpus = []\n",
        "label = []\n",
        "with ZipFile(dmba.get_data_file('AutoAndElectronics.zip')) as rawData:\n",
        "    for info in rawData.infolist():\n",
        "        if info.is_dir():\n",
        "            continue\n",
        "        label.append(1 if 'rec.autos' in info.filename else 0)\n",
        "        corpus.append(rawData.read(info))"
      ],
      "metadata": {
        "id": "U2ye8RhbzESN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print first item\n",
        "corpus[0]"
      ],
      "metadata": {
        "id": "GsPYjT4-CSJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5 minute exercise\n",
        "# Print first five items\n",
        "# Indicate whether they are about 'autos' or 'electronics'"
      ],
      "metadata": {
        "id": "MuUBlkf4EFru"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize\n",
        "\n",
        "count_vect = CountVectorizer(token_pattern='[a-zA-Z!:)]+', encoding='latin1')\n",
        "counts = count_vect.fit_transform(corpus)"
      ],
      "metadata": {
        "id": "YDgtqxT5zkV_"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\" \".join(count_vect.inverse_transform(counts[0])[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkaiCfo20Rr3",
        "outputId": "1245391e-9ea2-48fc-be35-3f60837ec6f1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "path: cantaloupe srv cs cmu edu!das news harvard edu!ogicse!uwm edu!wupost!uunet!brunix!cs brown edu!cs from: edu hok chung tsang) newsgroups: rec autos subject: re: saturn s pricing policy message id: apr date: : gmt article i d references: c oxwp kkm cso uiuc vir l r shuksan ds boeing com sender: organization: computer science dept lines: in fredd fred dickey) writes: carolinafan cka uxa edu) wrote: have been active defending lately on the net and would like to state my full opinion subject rather than just reply others points biggest problem some people seem be having is that dealers make k a car think most will agree with me comparably priced its competitors they aren t overpriced compared cars their class don understand point of arguing over whether dealer makes or not never understood what big deal profits either only thing can figure out believe if minimize profit total pocket expenses for while this may true cases do it generally bought sl january at time based studying prices decided there was no comparable as cheaply sure maybe could talked price other but wouldn any different important how much money left after buy reducing same saving money! show saves ll experience has does necessarily save say you your then paying so isn moreover really reduce margin by even better deals already below market average will: ) attract more saturns because them force lower survive now owners benefit from buyers pay less doug\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Magic code that does all preprocessing at once - stem, stop, etc.\n",
        "# Warning expected\n",
        "\n",
        "import nltk # Natural Language Toolkit\n",
        "nltk.download('punkt')\n",
        "from nltk import word_tokenize\n",
        "from nltk.stem.snowball import EnglishStemmer\n",
        "from sklearn.feature_extraction._stop_words import ENGLISH_STOP_WORDS\n",
        "\n",
        "class LemmaTokenizer(object):\n",
        "    def __init__(self):\n",
        "        self.stemmer = EnglishStemmer()\n",
        "        self.stopWords = set(ENGLISH_STOP_WORDS)\n",
        "\n",
        "    def __call__(self, doc):\n",
        "        return [self.stemmer.stem(t) for t in word_tokenize(doc)\n",
        "                if t.isalpha() and t not in self.stopWords]\n",
        "\n",
        "# Learn features based on text\n",
        "count_vect = CountVectorizer(tokenizer=LemmaTokenizer(), encoding='latin1')\n",
        "counts = count_vect.fit_transform(corpus)\n",
        "\n",
        "# Print first\n",
        "print(\" \".join(count_vect.inverse_transform(counts[0])[0]))\n"
      ],
      "metadata": {
        "id": "86jisiTk1fJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use more sophisticated version of frequency count\n",
        "# TFIDF - term frequency/ inverse document frequency\n",
        "# Gives more weight to words that are unique to a document\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "tfidfTransformer = TfidfTransformer()\n",
        "tfidf = tfidfTransformer.fit_transform(counts)"
      ],
      "metadata": {
        "id": "Y6EsGx4-HlgJ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf[0]"
      ],
      "metadata": {
        "id": "E6tdhMACER_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use only top 10 dimensions\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "svd = TruncatedSVD(10)\n",
        "normalizer = Normalizer(copy=False)\n",
        "lsa = make_pipeline(svd, normalizer)\n",
        "\n",
        "lsa_tfidf = lsa.fit_transform(tfidf)"
      ],
      "metadata": {
        "id": "WRX5jPebImS_"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lsa_tfidf[0]"
      ],
      "metadata": {
        "id": "d0FNCxC0Dg0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First five are all same category\n",
        "label[0:5]"
      ],
      "metadata": {
        "id": "rKBNS33xD1aS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from dmba import classificationSummary\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# Create training and testing set\n",
        "Xtrain, Xtest, ytrain, ytest = train_test_split(lsa_tfidf, label, test_size=0.4, random_state=42)\n",
        "\n",
        "\n",
        "# Initialize\n",
        "nn = MLPClassifier(random_state=42)\n",
        "\n",
        "# Fit model\n",
        "nn.fit(Xtrain, ytrain)\n",
        "\n",
        "classificationSummary(ytest, nn.predict(Xtest))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OQTHmFFFJGuS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Increase number of iterations (epochs)\n",
        "nn_long = MLPClassifier(max_iter=500, random_state=42)\n",
        "\n",
        "nn_long.fit(Xtrain, ytrain)\n",
        "\n",
        "classificationSummary(ytest, nn_long.predict(Xtest))"
      ],
      "metadata": {
        "id": "uRDvs_YeRlim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5 minute exercise\n",
        "# How many iterations are needed for it to converge?"
      ],
      "metadata": {
        "id": "yqI_CmanRhak"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Can also try decision trees\n",
        "\n",
        "# run decision tree model on training\n",
        "dt = DecisionTreeClassifier(random_state=1)\n",
        "dt.fit(Xtrain, ytrain)\n",
        "\n",
        "# print confusion matrix and accuracty\n",
        "classificationSummary(ytest, dt.predict(Xtest))\n",
        "\n",
        "# run random forest classifier model on training\n",
        "rf = RandomForestClassifier(n_estimators=500, random_state=1)\n",
        "rf.fit(Xtrain, ytrain)\n",
        "\n",
        "# print confusion matrix and accuracty\n",
        "classificationSummary(ytest, rf.predict(Xtest))"
      ],
      "metadata": {
        "id": "6wj3uRZGReyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decision trees slightly outperform neural networks. Can we improve the neural network?\n",
        "\n",
        "# Two hidden layers with 8 neurons each\n",
        "nn_2layer = MLPClassifier(hidden_layer_sizes=(8, 8), random_state=42)\n",
        "\n",
        "# Fit the model to the training data\n",
        "nn_2layer.fit(Xtrain, ytrain)\n",
        "\n",
        "classificationSummary(ytest, nn_2layer.predict(Xtest))"
      ],
      "metadata": {
        "id": "QaELP6pNSOqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5 minute Exercise\n",
        "# Can you find a neural network architecture that has an accuracy greater than 96%?\n",
        "# Vary the number of hidden layers and the number of neurons in each layer\n"
      ],
      "metadata": {
        "id": "VbloRMTtS22x"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Case study: Spam ads\n",
        "\n",
        "You are running an online agriculture site. Your revenue primarily comes from ads. To maintain your ethical standards, you do not want to run ads that are spam.\n",
        "\n",
        "Build a classifier that determines whether or not an ad is spam."
      ],
      "metadata": {
        "id": "J4i3vtJxvZoW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise: Examine farm-ads.csv.\n",
        "# What do you see?\n",
        "# What are the differences between spam ads (target: -1) and good ads (target: 1)?\n",
        "# What are the spam ads about? What words do they have in common?\n",
        "# What are the good ads about? What words do they have in common?"
      ],
      "metadata": {
        "id": "fhH3Cu7LwUva"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read in ads\n",
        "\n",
        "import dmba\n",
        "\n",
        "farm_ads = dmba.load_data('farm-ads.csv', names=['relevance', 'text'])\n",
        "\n",
        "print('{} relevant ads'.format(len(farm_ads[farm_ads.relevance == 1])))\n",
        "print(farm_ads[farm_ads.relevance == 1].head())\n",
        "\n",
        "print('{} non-relevant ads'.format(len(farm_ads[farm_ads.relevance == -1])))\n",
        "print(farm_ads[farm_ads.relevance == -1].head())"
      ],
      "metadata": {
        "id": "qK3wmeLaY3nl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize and create TFID\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "count_vect = CountVectorizer(token_pattern='[a-zA-Z-]+')\n",
        "tfidfTransformer = TfidfTransformer(smooth_idf=False, norm=None)\n",
        "\n",
        "counts = count_vect.fit_transform(farm_ads['text'])\n",
        "tfidf = tfidfTransformer.fit_transform(counts)"
      ],
      "metadata": {
        "id": "XB7z8wZKZik8"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Latent Semantic Analysis - 20 concepts\n",
        "\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "svd = TruncatedSVD(20)\n",
        "normalizer = Normalizer(copy=False)\n",
        "lsa = make_pipeline(svd, normalizer)\n",
        "\n",
        "lsa_tfidf = lsa.fit_transform(tfidf)"
      ],
      "metadata": {
        "id": "MhqkYgNeZ3Qq"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Examine term-document matrix\n",
        "# Remember that this matrix indicates which terms (tokens) are in which document\n",
        "\n",
        "shape = counts.shape\n",
        "print('Term-document matrix: {0[1]} terms, {0[0]} documents'.format(shape))\n",
        "print('   Size of the matrix: {}'.format(counts.size))\n",
        "print('  sparsity: {:.0f}%\\n'.format(100 * counts.size / (shape[0] * shape[1])))\n",
        "\n",
        "print(counts[:,0])"
      ],
      "metadata": {
        "id": "1a9Wm__jaMxx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build intuition by examining terms\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "fromTerm = 5340\n",
        "toTerm = 5350\n",
        "\n",
        "index = count_vect.get_feature_names_out()[fromTerm:toTerm]\n",
        "pd.DataFrame(data=counts[0:20,fromTerm:toTerm].toarray().transpose(), index=index)"
      ],
      "metadata": {
        "id": "FyDuvvOsLIOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 'add' occurs three times in document 7\n",
        "\n",
        "import numpy as np\n",
        "print('Document 7 - term 5344: ', counts[7, 5344])\n",
        "print('Document 8 - term 5344: ', counts[8, 5344])\n",
        "print('Average occurrence of term 5344 in all documents:', np.sum(counts[:, 5344]) / counts.shape[0])\n",
        "\n",
        "pd.Series([c.toarray()[0, 0] for c in counts[:, 5344]]).hist(bins=100)"
      ],
      "metadata": {
        "id": "QJtDMiuNLw3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise: Repeat this with a different term\n",
        "# Comment on the distribution of the term in the documents"
      ],
      "metadata": {
        "id": "HFMM7aoqNwji"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train neural network classifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from dmba import classificationSummary\n",
        "\n",
        "train_X, valid_X, train_y, valid_y = train_test_split(lsa_tfidf, farm_ads.relevance, test_size=0.4, random_state=42)\n",
        "\n",
        "neural_net = MLPClassifier(hidden_layer_sizes=(100,), activation='relu', solver='adam', max_iter=1000)\n",
        "\n",
        "# Train the neural network on the training data\n",
        "neural_net.fit(train_X, train_y)\n",
        "\n",
        "# Print confusion matrix and accuracy on the validation set\n",
        "classificationSummary(valid_y, neural_net.predict(valid_X), class_names=neural_net.classes_)"
      ],
      "metadata": {
        "id": "t9g4AQLcMHMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5 minute exercise\n",
        "# Improve the classification performance by changing the number of parameters\n",
        "# Try changing the number of hidden layers, the number of neurons in each layer"
      ],
      "metadata": {
        "id": "K4JeEWFDNZsB"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now let's try a simple decision tree\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "decision_tree = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "decision_tree.fit(train_X, train_y)\n",
        "\n",
        "\n",
        "classificationSummary(valid_y, decision_tree.predict(valid_X), class_names=decision_tree.classes_)"
      ],
      "metadata": {
        "id": "y6sX51rtOHHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise: Improve the classification performance by changing the hyperparameters\n",
        "# Try adjusting max_depth, min_samples_split\n"
      ],
      "metadata": {
        "id": "iAcu2JnQO7cm"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we will try a Random Forest!\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "random_forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "random_forest.fit(train_X, train_y)\n",
        "\n",
        "classificationSummary(valid_y, random_forest.predict(valid_X), class_names=random_forest.classes_)"
      ],
      "metadata": {
        "id": "UOdoOXr-POWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise: Improve the classification performance by changing the hyperparameters\n"
      ],
      "metadata": {
        "id": "Q7-OHZhYI4yc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Clustering\n",
        "\n"
      ],
      "metadata": {
        "id": "8J7gbBlwz4M5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "\n",
        "from zipfile import ZipFile\n",
        "import dmba\n",
        "\n",
        "corpus = []\n",
        "with ZipFile(dmba.get_data_file('AutoAndElectronics.zip')) as rawData:\n",
        "    for info in rawData.infolist():\n",
        "        if info.is_dir():\n",
        "            continue\n",
        "        corpus.append(rawData.read(info))"
      ],
      "metadata": {
        "id": "4_0swy6809UQ"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing: Stem, tokenize, stop words\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.stem.snowball import EnglishStemmer\n",
        "from sklearn.feature_extraction._stop_words import ENGLISH_STOP_WORDS\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk import word_tokenize\n",
        "\n",
        "class LemmaTokenizer(object):\n",
        "    def __init__(self):\n",
        "        self.stemmer = EnglishStemmer()\n",
        "        self.stopWords = set(ENGLISH_STOP_WORDS)\n",
        "\n",
        "    def __call__(self, doc):\n",
        "        return [self.stemmer.stem(t) for t in word_tokenize(doc)\n",
        "                if t.isalpha() and t not in self.stopWords]\n",
        "\n",
        "preprocessor = CountVectorizer(tokenizer=LemmaTokenizer(), stop_words='english', encoding='latin1')\n",
        "preprocessedText = preprocessor.fit_transform(corpus)"
      ],
      "metadata": {
        "id": "z7wZxzcsEP_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Latent Semantic Analysis\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "tfidfTransformer = TfidfTransformer()\n",
        "tfidf = tfidfTransformer.fit_transform(preprocessedText)"
      ],
      "metadata": {
        "id": "CBQmos5eFaP6"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create 10 concepts\n",
        "\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "svd = TruncatedSVD(10)\n",
        "normalizer = Normalizer(copy=False)\n",
        "lsa = make_pipeline(svd, normalizer)\n",
        "\n",
        "lsa_tfidf = lsa.fit_transform(tfidf)"
      ],
      "metadata": {
        "id": "Sg101anIFlMn"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hierarchical clustering\n",
        "\n",
        "from scipy.cluster.hierarchy import linkage\n",
        "\n",
        "Z = linkage(lsa_tfidf, method='average')"
      ],
      "metadata": {
        "id": "WkLSSprldJgA"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot hierarchical cluster\n",
        "# Takes about 15 seconds\n",
        "import matplotlib.pylab as plt\n",
        "from scipy.cluster.hierarchy import dendrogram\n",
        "\n",
        "fig = plt.figure(figsize=(10, 6))\n",
        "fig.subplots_adjust(bottom=0.23)\n",
        "plt.title('Hierarchical Clustering Dendrogram (Complete linkage)')\n",
        "plt.xlabel('Documents')\n",
        "dendrogram(Z, color_threshold=0.9)\n",
        "plt.axhline(y=8, color='black', linewidth=0.5, linestyle='dashed')\n",
        "plt.show()\n",
        "\n",
        "# Notice that there are two large clusters, corresponding to auto and electronics?"
      ],
      "metadata": {
        "id": "4Tpb18WrF3BW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Examine clusters\n",
        "from scipy.cluster.hierarchy import fcluster\n",
        "\n",
        "# Recluster with 20 clusters\n",
        "nclusters = 20\n",
        "membership = fcluster(Z, nclusters, criterion='maxclust')\n",
        "for clNumber in range(1, nclusters + 1):\n",
        "    nmembers = sum(membership == clNumber)\n",
        "    recAutos = ['Newsgroups: rec.autos' in str(doc) for doc, cl in zip(corpus, membership) if cl == clNumber]\n",
        "    ratioAutos = sum(recAutos) /nmembers\n",
        "    print(f'{sum(recAutos):3d} of {nmembers:3d} : {ratioAutos:.2f} {\"rec.autos\" if ratioAutos > 0.9 else \"\"}')"
      ],
      "metadata": {
        "id": "FePe_SPfG6vU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# K-means clustering with k=2\n",
        "from sklearn.cluster import KMeans\n",
        "import pandas as pd\n",
        "import math\n",
        "\n",
        "kmeans = KMeans(n_clusters=2, random_state=0, n_init='auto').fit(lsa_tfidf)\n",
        "\n"
      ],
      "metadata": {
        "id": "nIhPXSK8Hhih"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Between cluster distance\n",
        "centroids = pd.DataFrame(kmeans.cluster_centers_)\n",
        "print('Between-cluster distance: ', math.sqrt(sum(centroids.iloc[0, :] - centroids.iloc[1, :])**2))"
      ],
      "metadata": {
        "id": "kQLrzsRhH7Q7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Intra cluster distance\n",
        "\n",
        "withinClusterSS = [0] * 2\n",
        "clusterCount = [0] * 2\n",
        "for cluster, distance in zip(kmeans.labels_, kmeans.transform(lsa_tfidf)):\n",
        "    withinClusterSS[cluster] += distance[cluster]**2\n",
        "    clusterCount[cluster] += 1\n",
        "for cluster, withClustSS in enumerate(withinClusterSS):\n",
        "    count = clusterCount[cluster]\n",
        "    withinClusterDispersion = math.sqrt(withClustSS / (count - 1))\n",
        "    print(f'Cluster {cluster} ({count} members): {withinClusterDispersion:5.2f} within cluster')"
      ],
      "metadata": {
        "id": "mBwv8ALoH38R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}